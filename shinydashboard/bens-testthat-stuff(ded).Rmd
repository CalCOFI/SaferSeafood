```{r}
# LOAD LIBRARIES ----
library(testthat)
library(coda)



library(shiny)
library(shinydashboard)
library(shinyjs)
library(shinyWidgets)
library(tidyverse)
library(leaflet)
library(shinycssloaders)
library(extrafont)
library(showtext)
library(markdown)

library(rfishbase)


library(skimr)
library(tidymodels)
library(caret)
# Load the corrplot package
library(corrplot)
library(sjPlot)
library(rstanarm)

library(sf)
library(terra)
library(leaflet)

if("pacman" %in% installed.packages() == FALSE){install.packages("pacman")}
pacman::p_load(geojsonR, factoextra,sf,dplyr, ggplot2, maps, fields,raster,
               MuMIn, lubridate, tidyr,ggh4x, lme4,sdmTMB,inlabru,cowplot,marmap,sjPlot, tidyverse, plyr, tidybayes, brms, bayesplot, loo,ggeffects,
               DHARMa)
```

```{r}
# READ IN DATA ----

fish_data <- read_csv("data/fish_data_preprocessed.csv")
#view(fish_data)

fish_clean <- read_csv("data/fish_clean.csv")
```

```{r}
fish_clean_names <- as.data.frame(unique(fish_clean$scientific_name)) %>% 
  mutate(species = `unique(fish_clean$scientific_name)`) %>% 
  dplyr::select('species')


# get the unique species names - should be 61
species_name <- unique(fish_clean$scientific_name)

# Recode them into the names that are included in fishbase 

# update the names of the fish to match fishbase
species_name_clean <- as.data.frame(species_name) %>% 
  dplyr::mutate(species_name = case_when(species_name == "Embiotica jacksoni" ~ "Embiotoca jacksoni",
                                         species_name ==  "Rhinobatos productus" ~ "Pseudobatos productus",
                                         TRUE ~ species_name))

# load fish taxa
taxa <- rfishbase::load_taxa()

# filter the taxa based on name 
taxa_filter <- taxa %>% 
  filter(Species %in% species_name_clean$species_name) %>% 
  dplyr::select(scientific_name = Species, Family, Genus)

fish.clean.fam <- fish_clean %>% 
  dplyr::mutate(scientific_name = case_when(scientific_name == "Embiotica jacksoni" ~ "Embiotoca jacksoni",
                                            scientific_name ==  "Rhinobatos productus" ~ "Pseudobatos productus",
                                            TRUE ~ scientific_name)) %>%
  left_join(taxa_filter, by = "scientific_name") %>% 
  dplyr::mutate(Family = ifelse(scientific_name == "Doryteuthis opalescens",
                                "Loliginidae",
                                Family))

# load in dataframe with cleaned species name, and life history characterstics
fish_lh <- read_csv("data/species_common_science.csv") 
```



```{r}
# The model
brm.diet.habitat.year.fam.clean = readRDS(here::here("shinydashboard1", "data", "brm_mod.rda"))

model <- brm.diet.habitat.year.fam.clean
```




## Testing the Inputs and Outputs
Before getting into the statistical properties, make sure your Bayesian model accepts the correct inputs and returns outputs in the expected format:

Input Checks: Test for correct handling of input types, shapes, and error handling for invalid inputs.
Output Checks: Ensure that your function returns outputs in the correct format (e.g., lists, data frames) and includes all expected components (e.g., estimates, credible intervals).


1. Input Validation Test
Ensure that the model accepts the correct input data and handles potential errors appropriately:
```{r}
test_that("Bayesian model input validation", {
  # Assuming `fish.clean.fam` is your data
  expect_error(
    model, 
    "Error message indicating invalid input"
  )
})


```

2. Output Validation Tests
Check the properties of the model's outputs, such as convergence, parameter estimates, and model fit:
```{r}
library(rstan)  # Assuming output is from Stan, could also be from brms or JAGS

fit <- brm(
  formula = TotalDDT.trans.non | cens(Censored, Detection.Limit) ~ 
            TotalDDT.sed.trans * trophic_category +
            TotalDDT.sed.trans * feeding_position + 
            Year + (1 | Family), 
  data = fish.clean.fam, 
  prior = c(
    set_prior("cauchy(0, 0.5)", class = "b"),
    set_prior("cauchy(0, 2)", class = "sd")
  ),
  family = gaussian(),
  chains = 4, 
  iter = 2000
)

# Convert brms fit to coda's mcmc.list
mcmc_list <- as.mcmc(fit)

# Compute the Gelman-Rubin diagnostic using the coda package
gelman_results <- gelman.diag(mcmc_list)
print(gelman_results)
```
Purpose of Gelman-Rubin Diagnostic
The Gelman-Rubin diagnostic is used to evaluate the convergence of multiple chains in an MCMC sample. It is based on comparing the variance between chains to the variance within each chain. If all chains are converging to the same target distribution, the between-chain and within-chain variances should be similar.

How It Works
The Gelman-Rubin diagnostic calculates a scale reduction factor for each parameter in the model. This factor measures how much the scale of the current distribution might reduce if the chains were allowed to continue towards infinity. Ideally, if convergence has been achieved, the PSRF approaches 1.0. Values significantly greater than 1.0 suggest that the chains have not yet converged.

Using gelman.diag()
To use gelman.diag(), you need to pass it an object that contains the MCMC samples for each chain. The coda package expects this object to be of class mcmc.list. Here's a typical usage pattern:

Run multiple MCMC chains: Your Bayesian model, like those created using brms, JAGS, or Stan, should be run with multiple chains.
Create an mcmc.list object: Convert the output of these chains into an mcmc.list object if they are not already in this format.
Apply gelman.diag(): Pass this list to gelman.diag() to compute the diagnostics.

Interpreting Results
Values close to 1: Indicates that chains are likely well converged.
Values greater than 1.1: Suggest that more iterations might be necessary, or there may be issues with chain convergence or model specification.



Our results:
- Point estimate: The point estimate column shows the estimated potential scale reduction factor for each parameter.
- Upper C.I.: The upper confidence interval column provides the upper bound of the 95% confidence interval for the potential scale reduction factor.

Most of the parameters have PSRF values close to 1, indicating good convergence. A few parameters have PSRF values slightly above 1, but they are still within the range of good convergence. It's common to have some variability in the PSRF values, especially in larger and more complex models, but as long as the values are close to 1, you can generally trust the results of the Bayesian analysis.

The "Multivariate psrf" value represents the multivariate potential scale reduction factor, which summarizes the convergence across all parameters in the model. A value close to 1 indicates good overall convergence.

In summary, based on the PSRF values provided, your Bayesian model appears to have converged well, and the parameter estimates should be reliable. However, it's always a good practice to visually inspect diagnostic plots and perform additional checks to ensure the validity of your results.





### Posterior Predictive Checks
A common approach for checking the fit of a Bayesian model is to use posterior predictive checks (PPC), which compare observed data to data generated from the model.

```{r}
library(bayesplot)

# Generating posterior predictive checks
pp_check(fit, type = "hist")  # Histogram of observations vs. predictions

```
Observed Data Distribution: The darker histogram shows the actual data you collected and are modeling. This is your reference point for what the outcomes really look like.

Simulated Data Distributions: The lighter histograms or lines represent possible outcomes generated based on the model parameters learned during the Bayesian analysis. These simulations are based on the posterior distributions of your model's parameters and show the variability and uncertainty in model predictions.

Histograms: Posterior predictive checks generate histograms comparing observed data to data simulated from the posterior predictive distribution.
If the model fits the data well, the histograms of observed and simulated data should be similar.
Differences between observed and simulated data can indicate areas where the model is not capturing the data's distribution adequately.




### Leave-One-Out Cross-Validation (LOO)
LOO is used to estimate the out-of-sample predictive fit of the model. brms can calculate LOO using the loo package.

```{r}
library(loo)

# LOO cross-validation
loo_result <- loo(fit)
print(loo_result)

```
LOO Information Criterion (LOOIC): LOOIC is a measure of model fit that balances the trade-off between model complexity and goodness of fit.
Lower LOOIC values indicate better model fit.
Differences in LOOIC between models can be used for model comparison.

In your model:

1100 out of 1103 log-likelihoods fall in the good range, which is excellent.
3 log-likelihoods are in the OK range, suggesting minor issues with the model fit for these observations but generally still acceptable.
There are no observations in the bad or very bad categories, which is favorable for the overall reliability of the LOO results.

Estimates
**elpd_loo**: The estimated expected log pointwise predictive density, a measure of the model's predictive accuracy, is -1662.5. This is the sum of the expected log predictive densities for each observed data point, one at a time, using the other data points to fit the model.
**SE**: The standard error of the elpd_loo estimate is 27.7, indicating the uncertainty in the elpd_loo estimate.
**p_loo**: Effective number of parameters or complexity of the model, estimated at 29.7. This represents the model's capacity to fit varying patterns in the data. A higher p_loo suggests a more complex model.
**looic**: Leave-One-Out Information Criterion, computed as -2 * elpd_loo, is 3325.1 with a standard error of 55.5. This is a measure of model fit that penalizes for model complexity—lower values are better, similar to AIC and BIC in frequentist statistics.

Conclusion
Overall, the LOO output suggests that your model has a good out-of-sample predictive performance with reliable LOO estimates for nearly all observations. The effective number of parameters (p_loo) indicates a model that is complex enough to capture essential patterns without being overly fitted. These are positive signs of a well-specified Bayesian model.


### Convergence Diagnostics
You've already looked at PSRF, but it's good to also visually inspect trace plots for each parameter to ensure that they show good mixing and no signs of non-convergence.
```{r}
library(bayesplot)

# Trace plots
mcmc_trace(as.mcmc(fit), pars = c("b_Intercept", "b_Year"))

```
Trace Plots: Trace plots show the values of each parameter over iterations of the MCMC sampling process.
Good mixing is indicated by a "noisy" trace that fluctuates around the posterior mean.
Lack of mixing or convergence issues may be indicated by patterns such as trends, oscillations, or non-stationary behavior.





###Comparison to Prior Distributions
Comparing posterior distributions to the priors can help check if the data provide sufficient information about the parameters or if the posteriors are overly influenced by the priors.
```{r}
# Extract posterior samples from the brmsfit object
posterior_samples <- posterior_samples(fit)

# Visualize posterior distributions
mcmc_hist(posterior_samples, pars = c("b_Intercept", "b_TotalDDT.sed.trans"))

# Review prior distributions
prior_summary(fit)

```
Posterior vs. Prior: This plot compares the posterior distribution of each parameter to its corresponding prior distribution.
If the posterior distribution differs significantly from the prior, it suggests that the data have informed the parameter estimate.
Strong similarities may indicate that the prior is dominating the posterior, potentially raising concerns about the model's sensitivity to the choice of prior.

A smaller (narrower) histogram for b_TotalDDT.sed.trans compared to b_Intercept indicates a tighter, more certain estimate of this parameter's effect, while the broader histogram for b_Intercept suggests more uncertainty or variability in estimating the baseline effect in the model.






###Sensitivity Analysis
Changing the priors slightly or using different model specifications can help determine the sensitivity of the conclusions to the assumptions of the model.
```{r}
# Example: Adjusting a prior slightly
fit2 <- update(fit, prior = set_prior("cauchy(0, 0.75)", class = "b"))

# Compare the results of the original and updated model
print(summary(fit))
print(summary(fit2))

```
Comparison of Results: Comparing results between models with different priors or specifications can reveal how sensitive the conclusions are to changes in the model.
Consistency between models suggests robustness of conclusions.
Differences in results may highlight areas where the model's conclusions are sensitive to modeling choices.










## Performance Testing
For models that are computationally intensive, you might want to include performance tests:

Timing Tests: Ensure that the model runs within an acceptable time frame. This is useful for regression testing to ensure performance hasn’t degraded.

```{r}
test_that("Bayesian model performance", {
  start_time <- Sys.time()
  model
  end_time <- Sys.time()
  expect_true(difftime(end_time, start_time, units = "secs") < 60, "Should run in less than 60 seconds")
})

```



## Testing the Statistical Assumptions
Posterior Distribution Checks: Ensure the posterior distributions of parameters make sense given the data and prior knowledge. This might include checking moments (mean, variance) or tail behavior.
Predictive Checks: Use posterior predictive checks to see if simulated data from the model aligns well with the observed data.

```{r}
test_that("Posterior predictive checks", {
  set.seed(123)
  result <- model(data)
  simulated_data <- simulate_from_posterior(result$posterior, n = 100)
  # Assuming some function to compare datasets
  comparison_result <- compare_datasets(simulated_data, data)
  expect_true(comparison_result$statistic < some_threshold)
})
```


## Testing Convergence and Stability
Bayesian models typically involve Markov Chain Monte Carlo (MCMC) or similar algorithms that need to converge to give meaningful results:

Convergence Tests: Check if the MCMC chains are converging. This can be done by examining trace plots programmatically or using diagnostic statistics like the Gelman-Rubin statistic.
Stability Tests: Re-run the model with the same inputs to see if the results are consistent (to the extent expected for stochastic processes). This might involve setting a seed for reproducibility in tests.

```{r}
test_that("Bayesian model converges correctly", {
  set.seed(123)
  result <- model(data)
  # Use `coda` package for convergence diagnostics
  library(coda)
  mcmc_obj <- as.mcmc(result$samples)
  gelman_diag <- gelman.diag(mcmc_obj)
  expect_true(gelman_diag$mpsrf < 1.1, "Chains should converge (Gelman-Rubin < 1.1)")
})

```


## Cross-Validation
Perform cross-validation to assess the model's predictive performance:
```{r}
test_that("Cross-validation", {
  # Assuming `cv` is a function for cross-validation
  cv_results <- cv(model = fit, method = "LOO")
  
  # Check cross-validation results
  expect_true(all(cv_results$looic < fit$looic), "LOOIC should be lower for cross-validated model")
})
```

## Robustness Checks
Test the robustness of the model to outliers or influential observations:
```{r}
test_that("Robustness checks", {
  # Introduce outliers or influential observations
  data_with_outliers <- introduce_outliers(fish.clean.fam)
  
  # Fit the model with the modified data
  fit_with_outliers <- brm(..., data = data_with_outliers, prior = c(...))
  
  # Check if the model's conclusions remain robust
  expect_true(compare_results(fit, fit_with_outliers), "Model should remain robust to outliers")
})
```


## Boundary Checks
Check how the model behaves near the boundaries of the parameter space or data range:
```{r}
test_that("Boundary checks", {
  # Generate data near the boundaries
  data_boundary <- generate_boundary_data(...)
  
  # Fit the model with the boundary data
  fit_boundary <- brm(..., data = data_boundary, prior = c(...))
  
  # Check if the model behaves as expected near the boundaries
  expect_true(validate_boundary_behavior(fit_boundary), "Model should behave appropriately near boundaries")
})

```


